import OpenAI from "openai";
import {StdioMcpClientToFunction} from "./StdioMcpServerToFunction"
import type { ChatCompletionMessageParam } from "openai/resources/chat/completions";

export class LlmAgent {
    private modelName: string = '';
    private roleMessages: ChatCompletionMessageParam[];
    private messages: ChatCompletionMessageParam[];
    private openai: OpenAI;
    private client: StdioMcpClientToFunction;
    /** å•ä¾‹å®ä¾‹ */
    private static instance: LlmAgent;

    constructor(client:StdioMcpClientToFunction,baseUrl: string, apiKey: string, modelName: string,sysPrompt?: string) {
        if (!baseUrl) {
            console.log("baseUrl is required");
            return;
        }
        if (!apiKey) {
            console.log("apiKey is required");
            return;
        }
        if (!modelName) {
            console.log("model is required");
            return;
        }
        console.log("Creating LlmAgent instance...");
        console.log(`${baseUrl}, ${apiKey}, ${modelName}`);
        this.modelName = modelName;
        this.client = client;
        this.openai = new OpenAI({
            apiKey: apiKey || '',
            baseURL: baseUrl || '',
        });
        this.roleMessages = [
            {
                role: "system",
                content: sysPrompt || "You are a helpful assistant that can answer questions and help with tasks."
            },
        ];
    }

    /**
     * è·å–å•ä¾‹å®ä¾‹
     * 
     * @returns {StdioMcpClientToFunction} å•ä¾‹å®ä¾‹
     */
    static getInstance(client: StdioMcpClientToFunction,baseUrl: string, apiKey: string, model: string,sysPrompt?: string): LlmAgent {
        if (!this.instance) {
            this.instance = new LlmAgent(client,baseUrl, apiKey, model,sysPrompt);
        }
        return this.instance;
    }


    private async handleToolCalls(response: OpenAI.Chat.Completions.ChatCompletion, messages: ChatCompletionMessageParam[]) {
        let currentResponse = response;
        let counter = 0; // é¿å…é‡å¤æ‰“å° AI çš„å“åº”æ¶ˆæ¯

        // å¤„ç†å·¥å…·è°ƒç”¨, ç›´åˆ°æ²¡æœ‰å·¥å…·è°ƒç”¨
        while (currentResponse.choices[0].message.tool_calls) {
            // æ‰“å°å½“å‰ AI çš„å“åº”æ¶ˆæ¯
            if (currentResponse.choices[0].message.content && counter !== 0) {
                console.log("\nğŸ¤– AI:", currentResponse.choices[0].message.content);
            }
            counter++;

            for (const toolCall of currentResponse.choices[0].message.tool_calls) {
                const toolName = toolCall.function.name;
                const toolArgs = JSON.parse(toolCall.function.arguments);

                console.log(`\nğŸ”§ è°ƒç”¨å·¥å…· ${toolName}`);
                console.log(`ğŸ“ å‚æ•°:`, JSON.stringify(toolArgs, null, 2));
                console.log(toolName);
                console.log(toolArgs);

                // æ‰§è¡Œå·¥å…·è°ƒç”¨
                const result = await this.client.callTool(toolName, toolArgs);

                // æ·»åŠ  AI çš„å“åº”å’Œå·¥å…·è°ƒç”¨ç»“æœåˆ°æ¶ˆæ¯å†å²
                messages.push(currentResponse.choices[0].message);
                messages.push({
                    role: "tool",
                    tool_call_id: toolCall.id,
                    content: JSON.stringify(result.content),
                } as ChatCompletionMessageParam);
            }

            // è·å–ä¸‹ä¸€ä¸ªå“åº”
            currentResponse = await this.openai.chat.completions.create({
                model: this.modelName || 'deepseek-chat',
                messages: messages,
                tools: this.client.allMcpServer.funcTools,
            });
        }

        return currentResponse;
    }


    async llmQuery(messages: ChatCompletionMessageParam[]): Promise<ChatCompletionMessageParam[]> {
        // æ·»åŠ åˆ°meesages
        if (!messages || (messages && messages.length === 0)) {
            return [];
        }
        console.log("ğŸ’¬ ç”¨æˆ·:",messages);
        if (messages[0].role === "system" && messages[0].content === this.roleMessages[0].content) {
            this.messages = messages;
        } else {
            this.messages = [...this.roleMessages, ...messages];
        }


        // å¦‚æœå°šæœªåŠ è½½æ‰€æœ‰ MCP æœåŠ¡å™¨ä¿¡æ¯ï¼Œå…ˆåŠ è½½
        if (!this.client.allMcpServer) {
            await this.client.fetchAllMcpServerData();
        }

        // åˆå§‹ OpenAI API è°ƒç”¨
        let response = await this.openai.chat.completions.create({
            model: this.modelName || 'deepseek-chat',
            messages: this.messages,
            tools: this.client.allMcpServer.funcTools,
        });

        // æ‰“å°åˆå§‹å“åº”æ¶ˆæ¯
        if (response.choices[0].message.content) {
            console.log("\nğŸ¤– AI:", response.choices[0].message.content);
        }

        // å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œå¤„ç†å®ƒä»¬
        if (response.choices[0].message.tool_calls) {
            response = await this.handleToolCalls(response, this.messages);
        }
        
        // å°†æœ€ç»ˆå“åº”æ·»åŠ åˆ°æ¶ˆæ¯å†å²
        this.messages.push(response.choices[0].message);

        return this.messages;
    }
}